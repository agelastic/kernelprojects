From e73edac75c512c5f34c8ed0244120c66797e61a0 Mon Sep 17 00:00:00 2001
From: Vitaly Osipov <vitaly.osipov@gmail.com>
Date: Tue, 13 May 2014 21:52:22 +1000
Subject: [PATCH] Make SLOB use best-fit algorithm inside a page

Modified SLOB allocator to use best-fit algorithm when allocating a
block inside slob_page_alloc. Also added two syscalls to obtain the
amount of memory actually used vs reserved.

Signed-off-by: Vitaly Osipov <vitaly.osipov@gmail.com>
---
 Makefile                         |   2 +-
 arch/x86/syscalls/syscall_64.tbl |   2 +
 include/linux/syscalls.h         |   2 +
 mm/slob.c                        | 100 +++++++++++++++++++++++++++------------
 4 files changed, 74 insertions(+), 32 deletions(-)

diff --git a/Makefile b/Makefile
index 7046756..257e2ff 100644
--- a/Makefile
+++ b/Makefile
@@ -1,7 +1,7 @@
 VERSION = 3
 PATCHLEVEL = 15
 SUBLEVEL = 0
-EXTRAVERSION = -rc4
+EXTRAVERSION = -rc4-slob-bf
 NAME = Shuffling Zombie Juror
 
 # *DOCUMENTATION*
diff --git a/arch/x86/syscalls/syscall_64.tbl b/arch/x86/syscalls/syscall_64.tbl
index ec255a1..00506fc 100644
--- a/arch/x86/syscalls/syscall_64.tbl
+++ b/arch/x86/syscalls/syscall_64.tbl
@@ -323,6 +323,8 @@
 314	common	sched_setattr		sys_sched_setattr
 315	common	sched_getattr		sys_sched_getattr
 316	common	renameat2		sys_renameat2
+317     common  slob_claimed            sys_slob_claimed
+318     common  slob_used               sys_slob_used
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index b0881a0..b577676 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -866,4 +866,6 @@ asmlinkage long sys_process_vm_writev(pid_t pid,
 asmlinkage long sys_kcmp(pid_t pid1, pid_t pid2, int type,
 			 unsigned long idx1, unsigned long idx2);
 asmlinkage long sys_finit_module(int fd, const char __user *uargs, int flags);
+asmlinkage long sys_slob_claimed(void);
+asmlinkage long sys_slob_used(void);
 #endif
diff --git a/mm/slob.c b/mm/slob.c
index 21980e0..4da3be7 100644
--- a/mm/slob.c
+++ b/mm/slob.c
@@ -67,6 +67,7 @@
 #include <linux/rcupdate.h>
 #include <linux/list.h>
 #include <linux/kmemleak.h>
+#include <linux/syscalls.h>
 
 #include <trace/events/kmem.h>
 
@@ -101,6 +102,8 @@ static LIST_HEAD(free_slob_small);
 static LIST_HEAD(free_slob_medium);
 static LIST_HEAD(free_slob_large);
 
+static unsigned int mem_total, mem_used;
+
 /*
  * slob_page_free: true for pages on free_slob_pages list.
  */
@@ -219,6 +222,10 @@ static void *slob_page_alloc(struct page *sp, size_t size, int align)
 	slob_t *prev, *cur, *aligned = NULL;
 	int delta = 0, units = SLOB_UNITS(size);
 
+	slob_t *bprev = NULL, *bcur = NULL, *baligned = NULL;
+	int bdelta = 0;
+	slobidx_t bavail = SLOB_UNITS(PAGE_SIZE);
+
 	for (prev = NULL, cur = sp->freelist; ; prev = cur, cur = slob_next(cur)) {
 		slobidx_t avail = slob_units(cur);
 
@@ -226,39 +233,53 @@ static void *slob_page_alloc(struct page *sp, size_t size, int align)
 			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
 			delta = aligned - cur;
 		}
-		if (avail >= units + delta) { /* room enough? */
-			slob_t *next;
-
-			if (delta) { /* need to fragment head to align? */
-				next = slob_next(cur);
-				set_slob(aligned, avail - delta, next);
-				set_slob(cur, delta, aligned);
-				prev = cur;
-				cur = aligned;
-				avail = slob_units(cur);
-			}
-
-			next = slob_next(cur);
-			if (avail == units) { /* exact fit? unlink. */
-				if (prev)
-					set_slob(prev, slob_units(prev), next);
-				else
-					sp->freelist = next;
-			} else { /* fragment */
-				if (prev)
-					set_slob(prev, slob_units(prev), cur + units);
-				else
-					sp->freelist = cur + units;
-				set_slob(cur + units, avail - units, next);
+		if (avail >= units + delta &&
+		    (!bcur || avail < bavail)) { /* room enough? */
+			bprev = prev;
+			bcur = cur;
+			baligned = aligned;
+			bdelta = delta;
+			bavail = avail;
+		}
+		if (slob_last(cur)) {
+			if (bcur) {
+
+				slob_t *bnext = NULL;
+
+				if (bdelta) {
+					/* need to fragment head to align? */
+					bnext = slob_next(bcur);
+					set_slob(baligned, bavail-bdelta, bnext);
+					set_slob(bcur, bdelta, baligned);
+					bprev = bcur;
+					bcur = baligned;
+					bavail = slob_units(bcur);
+				}
+
+				bnext = slob_next(bcur);
+				if (bavail == units) { /* exact fit? unlink. */
+					if (bprev)
+						set_slob(bprev,
+							 slob_units(bprev),
+							 bnext);
+					else
+						sp->freelist = bnext;
+				} else { /* fragment */
+					if (bprev)
+						set_slob(bprev,
+							 slob_units(bprev),
+							 bcur + units);
+					else
+						sp->freelist = bcur + units;
+					set_slob(bcur + units, bavail-units, bnext);
+				}
+
+				sp->units -= units;
+				if (!sp->units)
+					clear_slob_page_free(sp);
 			}
-
-			sp->units -= units;
-			if (!sp->units)
-				clear_slob_page_free(sp);
-			return cur;
+			return bcur;
 		}
-		if (slob_last(cur))
-			return NULL;
 	}
 }
 
@@ -307,6 +328,8 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 		if (prev != slob_list->prev &&
 				slob_list->next != prev->next)
 			list_move_tail(slob_list, prev->next);
+
+		mem_used += size;
 		break;
 	}
 	spin_unlock_irqrestore(&slob_lock, flags);
@@ -316,6 +339,7 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 		b = slob_new_pages(gfp & ~__GFP_ZERO, 0, node);
 		if (!b)
 			return NULL;
+		mem_total += PAGE_SIZE;
 		sp = virt_to_page(b);
 		__SetPageSlab(sp);
 
@@ -327,6 +351,7 @@ static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 		set_slob_page_free(sp, slob_list);
 		b = slob_page_alloc(sp, size, align);
 		BUG_ON(!b);
+		mem_used += size;
 		spin_unlock_irqrestore(&slob_lock, flags);
 	}
 	if (unlikely((gfp & __GFP_ZERO) && b))
@@ -362,6 +387,8 @@ static void slob_free(void *block, int size)
 		__ClearPageSlab(sp);
 		page_mapcount_reset(sp);
 		slob_free_pages(b, 0);
+		mem_total -= PAGE_SIZE;
+		mem_used -= size;
 		return;
 	}
 
@@ -416,6 +443,7 @@ static void slob_free(void *block, int size)
 			set_slob(prev, slob_units(prev), b);
 	}
 out:
+	mem_used -= size;
 	spin_unlock_irqrestore(&slob_lock, flags);
 }
 
@@ -642,3 +670,13 @@ void __init kmem_cache_init_late(void)
 {
 	slab_state = FULL;
 }
+
+SYSCALL_DEFINE0(slob_claimed)
+{
+	return mem_total;
+}
+
+SYSCALL_DEFINE0(slob_used)
+{
+	return mem_used;
+}
-- 
1.9.1

